[![Version](https://img.shields.io/pypi/v/libre-chat)](https://pypi.org/project/libre-chat) [![Python versions](https://img.shields.io/pypi/pyversions/libre-chat)](https://pypi.org/project/libre-chat) [![MIT license](https://img.shields.io/pypi/l/libre-chat)](https://github.com/vemonet/libre-chat/blob/main/LICENSE)

`libre-chat` is a Python library.

API and UI to deploy LLM models

## ‚ÑπÔ∏è How it works

You can use it with a CLI, or as an object in a python script.

!!! help "Report issues"

    Feel free to create [issues on GitHub](https://github.com/vemonet/libre-chat/issues){:target="_blank"}, if you are facing problems, have a question, or would like to see a feature implemented. Pull requests are welcome!

<!--
## üóÉÔ∏è Projects using libre-chat

Here are some projects using `libre-chat`:

* TODO
-->

## ü§ù Credits

Inspired by:
- https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference
- https://github.com/liltom-eth/llama2-webui

Library built with [MaastrichtU-IDS/cookiecutter-python-package](https://github.com/MaastrichtU-IDS/cookiecutter-python-package){:target="_blank"}.
