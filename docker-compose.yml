version: "3"
services:

  libre-chat:
    build: .
    # image: ghcr.io/vemonet/libre-chat:main
    volumes:
      - ./:/data # Share the current directory with chat.yml, models, vectorstore
    shm_size: '16g'
    environment:
      - LIBRECHAT_WORKERS=4
      # For deployment with nginx-proxy https://github.com/nginx-proxy/nginx-proxy
      - VIRTUAL_HOST=chat.semanticscience.org
      - LETSENCRYPT_HOST=chat.semanticscience.org
      - VIRTUAL_PORT=8000
      # - CUDA_VISIBLE_DEVICES=0 # Limit which GPU is made available
    # ports:
    #   - 8000:8000
    # entrypoint: uvicorn scripts.main:app --reload
    # deploy:  # Enable GPU in the container
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

    # networks:
    #   - no-internet

# networks:
#   no-internet:
#     driver: bridge
#     internal: true
